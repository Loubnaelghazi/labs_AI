{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9282871,"sourceType":"datasetVersion","datasetId":5618912}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression  #for binary classification is more useful\nfrom sklearn.metrics import accuracy_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-30T16:15:40.767387Z","iopub.execute_input":"2024-08-30T16:15:40.767998Z","iopub.status.idle":"2024-08-30T16:15:42.871832Z","shell.execute_reply.started":"2024-08-30T16:15:40.767936Z","shell.execute_reply":"2024-08-30T16:15:42.870411Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"data= pd.read_csv('/kaggle/input/dataset/Copy of sonar data.csv',header=None) #OUR columns don t have names","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:18:51.843361Z","iopub.execute_input":"2024-08-30T16:18:51.843841Z","iopub.status.idle":"2024-08-30T16:18:51.857005Z","shell.execute_reply.started":"2024-08-30T16:18:51.843793Z","shell.execute_reply":"2024-08-30T16:18:51.855651Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\ndata.head(6)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:19:57.043232Z","iopub.execute_input":"2024-08-30T16:19:57.043712Z","iopub.status.idle":"2024-08-30T16:19:57.078480Z","shell.execute_reply.started":"2024-08-30T16:19:57.043667Z","shell.execute_reply":"2024-08-30T16:19:57.077077Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"       0       1       2       3       4       5       6       7       8   \\\n0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n5  0.0286  0.0453  0.0277  0.0174  0.0384  0.0990  0.1201  0.1833  0.2105   \n\n       9   ...      51      52      53      54      55      56      57  \\\n0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n5  0.3039  ...  0.0045  0.0014  0.0038  0.0013  0.0089  0.0057  0.0027   \n\n       58      59  60  \n0  0.0090  0.0032   R  \n1  0.0052  0.0044   R  \n2  0.0095  0.0078   R  \n3  0.0040  0.0117   R  \n4  0.0107  0.0094   R  \n5  0.0051  0.0062   R  \n\n[6 rows x 61 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>55</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n      <th>60</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0200</td>\n      <td>0.0371</td>\n      <td>0.0428</td>\n      <td>0.0207</td>\n      <td>0.0954</td>\n      <td>0.0986</td>\n      <td>0.1539</td>\n      <td>0.1601</td>\n      <td>0.3109</td>\n      <td>0.2111</td>\n      <td>...</td>\n      <td>0.0027</td>\n      <td>0.0065</td>\n      <td>0.0159</td>\n      <td>0.0072</td>\n      <td>0.0167</td>\n      <td>0.0180</td>\n      <td>0.0084</td>\n      <td>0.0090</td>\n      <td>0.0032</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0453</td>\n      <td>0.0523</td>\n      <td>0.0843</td>\n      <td>0.0689</td>\n      <td>0.1183</td>\n      <td>0.2583</td>\n      <td>0.2156</td>\n      <td>0.3481</td>\n      <td>0.3337</td>\n      <td>0.2872</td>\n      <td>...</td>\n      <td>0.0084</td>\n      <td>0.0089</td>\n      <td>0.0048</td>\n      <td>0.0094</td>\n      <td>0.0191</td>\n      <td>0.0140</td>\n      <td>0.0049</td>\n      <td>0.0052</td>\n      <td>0.0044</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0262</td>\n      <td>0.0582</td>\n      <td>0.1099</td>\n      <td>0.1083</td>\n      <td>0.0974</td>\n      <td>0.2280</td>\n      <td>0.2431</td>\n      <td>0.3771</td>\n      <td>0.5598</td>\n      <td>0.6194</td>\n      <td>...</td>\n      <td>0.0232</td>\n      <td>0.0166</td>\n      <td>0.0095</td>\n      <td>0.0180</td>\n      <td>0.0244</td>\n      <td>0.0316</td>\n      <td>0.0164</td>\n      <td>0.0095</td>\n      <td>0.0078</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0100</td>\n      <td>0.0171</td>\n      <td>0.0623</td>\n      <td>0.0205</td>\n      <td>0.0205</td>\n      <td>0.0368</td>\n      <td>0.1098</td>\n      <td>0.1276</td>\n      <td>0.0598</td>\n      <td>0.1264</td>\n      <td>...</td>\n      <td>0.0121</td>\n      <td>0.0036</td>\n      <td>0.0150</td>\n      <td>0.0085</td>\n      <td>0.0073</td>\n      <td>0.0050</td>\n      <td>0.0044</td>\n      <td>0.0040</td>\n      <td>0.0117</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0762</td>\n      <td>0.0666</td>\n      <td>0.0481</td>\n      <td>0.0394</td>\n      <td>0.0590</td>\n      <td>0.0649</td>\n      <td>0.1209</td>\n      <td>0.2467</td>\n      <td>0.3564</td>\n      <td>0.4459</td>\n      <td>...</td>\n      <td>0.0031</td>\n      <td>0.0054</td>\n      <td>0.0105</td>\n      <td>0.0110</td>\n      <td>0.0015</td>\n      <td>0.0072</td>\n      <td>0.0048</td>\n      <td>0.0107</td>\n      <td>0.0094</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.0286</td>\n      <td>0.0453</td>\n      <td>0.0277</td>\n      <td>0.0174</td>\n      <td>0.0384</td>\n      <td>0.0990</td>\n      <td>0.1201</td>\n      <td>0.1833</td>\n      <td>0.2105</td>\n      <td>0.3039</td>\n      <td>...</td>\n      <td>0.0045</td>\n      <td>0.0014</td>\n      <td>0.0038</td>\n      <td>0.0013</td>\n      <td>0.0089</td>\n      <td>0.0057</td>\n      <td>0.0027</td>\n      <td>0.0051</td>\n      <td>0.0062</td>\n      <td>R</td>\n    </tr>\n  </tbody>\n</table>\n<p>6 rows × 61 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.describe()  #statistical measures of the data","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:20:48.002577Z","iopub.execute_input":"2024-08-30T16:20:48.003120Z","iopub.status.idle":"2024-08-30T16:20:48.165040Z","shell.execute_reply.started":"2024-08-30T16:20:48.003063Z","shell.execute_reply":"2024-08-30T16:20:48.163575Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"               0           1           2           3           4           5   \\\ncount  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \nmean     0.029164    0.038437    0.043832    0.053892    0.075202    0.104570   \nstd      0.022991    0.032960    0.038428    0.046528    0.055552    0.059105   \nmin      0.001500    0.000600    0.001500    0.005800    0.006700    0.010200   \n25%      0.013350    0.016450    0.018950    0.024375    0.038050    0.067025   \n50%      0.022800    0.030800    0.034300    0.044050    0.062500    0.092150   \n75%      0.035550    0.047950    0.057950    0.064500    0.100275    0.134125   \nmax      0.137100    0.233900    0.305900    0.426400    0.401000    0.382300   \n\n               6           7           8           9   ...          50  \\\ncount  208.000000  208.000000  208.000000  208.000000  ...  208.000000   \nmean     0.121747    0.134799    0.178003    0.208259  ...    0.016069   \nstd      0.061788    0.085152    0.118387    0.134416  ...    0.012008   \nmin      0.003300    0.005500    0.007500    0.011300  ...    0.000000   \n25%      0.080900    0.080425    0.097025    0.111275  ...    0.008425   \n50%      0.106950    0.112100    0.152250    0.182400  ...    0.013900   \n75%      0.154000    0.169600    0.233425    0.268700  ...    0.020825   \nmax      0.372900    0.459000    0.682800    0.710600  ...    0.100400   \n\n               51          52          53          54          55          56  \\\ncount  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \nmean     0.013420    0.010709    0.010941    0.009290    0.008222    0.007820   \nstd      0.009634    0.007060    0.007301    0.007088    0.005736    0.005785   \nmin      0.000800    0.000500    0.001000    0.000600    0.000400    0.000300   \n25%      0.007275    0.005075    0.005375    0.004150    0.004400    0.003700   \n50%      0.011400    0.009550    0.009300    0.007500    0.006850    0.005950   \n75%      0.016725    0.014900    0.014500    0.012100    0.010575    0.010425   \nmax      0.070900    0.039000    0.035200    0.044700    0.039400    0.035500   \n\n               57          58          59  \ncount  208.000000  208.000000  208.000000  \nmean     0.007949    0.007941    0.006507  \nstd      0.006470    0.006181    0.005031  \nmin      0.000300    0.000100    0.000600  \n25%      0.003600    0.003675    0.003100  \n50%      0.005800    0.006400    0.005300  \n75%      0.010350    0.010325    0.008525  \nmax      0.044000    0.036400    0.043900  \n\n[8 rows x 60 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>55</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>...</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.029164</td>\n      <td>0.038437</td>\n      <td>0.043832</td>\n      <td>0.053892</td>\n      <td>0.075202</td>\n      <td>0.104570</td>\n      <td>0.121747</td>\n      <td>0.134799</td>\n      <td>0.178003</td>\n      <td>0.208259</td>\n      <td>...</td>\n      <td>0.016069</td>\n      <td>0.013420</td>\n      <td>0.010709</td>\n      <td>0.010941</td>\n      <td>0.009290</td>\n      <td>0.008222</td>\n      <td>0.007820</td>\n      <td>0.007949</td>\n      <td>0.007941</td>\n      <td>0.006507</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.022991</td>\n      <td>0.032960</td>\n      <td>0.038428</td>\n      <td>0.046528</td>\n      <td>0.055552</td>\n      <td>0.059105</td>\n      <td>0.061788</td>\n      <td>0.085152</td>\n      <td>0.118387</td>\n      <td>0.134416</td>\n      <td>...</td>\n      <td>0.012008</td>\n      <td>0.009634</td>\n      <td>0.007060</td>\n      <td>0.007301</td>\n      <td>0.007088</td>\n      <td>0.005736</td>\n      <td>0.005785</td>\n      <td>0.006470</td>\n      <td>0.006181</td>\n      <td>0.005031</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.001500</td>\n      <td>0.000600</td>\n      <td>0.001500</td>\n      <td>0.005800</td>\n      <td>0.006700</td>\n      <td>0.010200</td>\n      <td>0.003300</td>\n      <td>0.005500</td>\n      <td>0.007500</td>\n      <td>0.011300</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000800</td>\n      <td>0.000500</td>\n      <td>0.001000</td>\n      <td>0.000600</td>\n      <td>0.000400</td>\n      <td>0.000300</td>\n      <td>0.000300</td>\n      <td>0.000100</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.013350</td>\n      <td>0.016450</td>\n      <td>0.018950</td>\n      <td>0.024375</td>\n      <td>0.038050</td>\n      <td>0.067025</td>\n      <td>0.080900</td>\n      <td>0.080425</td>\n      <td>0.097025</td>\n      <td>0.111275</td>\n      <td>...</td>\n      <td>0.008425</td>\n      <td>0.007275</td>\n      <td>0.005075</td>\n      <td>0.005375</td>\n      <td>0.004150</td>\n      <td>0.004400</td>\n      <td>0.003700</td>\n      <td>0.003600</td>\n      <td>0.003675</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.022800</td>\n      <td>0.030800</td>\n      <td>0.034300</td>\n      <td>0.044050</td>\n      <td>0.062500</td>\n      <td>0.092150</td>\n      <td>0.106950</td>\n      <td>0.112100</td>\n      <td>0.152250</td>\n      <td>0.182400</td>\n      <td>...</td>\n      <td>0.013900</td>\n      <td>0.011400</td>\n      <td>0.009550</td>\n      <td>0.009300</td>\n      <td>0.007500</td>\n      <td>0.006850</td>\n      <td>0.005950</td>\n      <td>0.005800</td>\n      <td>0.006400</td>\n      <td>0.005300</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.035550</td>\n      <td>0.047950</td>\n      <td>0.057950</td>\n      <td>0.064500</td>\n      <td>0.100275</td>\n      <td>0.134125</td>\n      <td>0.154000</td>\n      <td>0.169600</td>\n      <td>0.233425</td>\n      <td>0.268700</td>\n      <td>...</td>\n      <td>0.020825</td>\n      <td>0.016725</td>\n      <td>0.014900</td>\n      <td>0.014500</td>\n      <td>0.012100</td>\n      <td>0.010575</td>\n      <td>0.010425</td>\n      <td>0.010350</td>\n      <td>0.010325</td>\n      <td>0.008525</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.137100</td>\n      <td>0.233900</td>\n      <td>0.305900</td>\n      <td>0.426400</td>\n      <td>0.401000</td>\n      <td>0.382300</td>\n      <td>0.372900</td>\n      <td>0.459000</td>\n      <td>0.682800</td>\n      <td>0.710600</td>\n      <td>...</td>\n      <td>0.100400</td>\n      <td>0.070900</td>\n      <td>0.039000</td>\n      <td>0.035200</td>\n      <td>0.044700</td>\n      <td>0.039400</td>\n      <td>0.035500</td>\n      <td>0.044000</td>\n      <td>0.036400</td>\n      <td>0.043900</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 60 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data[60].value_counts() #the column that contains R AND M (ROCK AND MINE)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:23:29.133567Z","iopub.execute_input":"2024-08-30T16:23:29.134208Z","iopub.status.idle":"2024-08-30T16:23:29.149779Z","shell.execute_reply.started":"2024-08-30T16:23:29.134146Z","shell.execute_reply":"2024-08-30T16:23:29.148546Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"60\nM    111\nR     97\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"#SO THERE S 111 MINES AND 97 ROCKS, OURS CLASSES ARE ALMOST EQUAL . GOOD ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"WE DON T NEED TO VISUALIZE OUR DATA, OT S CLEAN , EQUAL , AND NO MISSING OR UNWANTED VALUES ARE DETECTED SO LET S SPLIT OUR DATA TO THE NEXT PHASE !","metadata":{}},{"cell_type":"code","source":"#seprating data and labels (supervised learning)\nX = data.drop(columns=60, axis=1) #axis =1 to acces to the columns \nY = data[60]","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:31:36.324011Z","iopub.execute_input":"2024-08-30T16:31:36.324453Z","iopub.status.idle":"2024-08-30T16:31:36.334127Z","shell.execute_reply.started":"2024-08-30T16:31:36.324410Z","shell.execute_reply":"2024-08-30T16:31:36.332686Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(X)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:31:54.874894Z","iopub.execute_input":"2024-08-30T16:31:54.875341Z","iopub.status.idle":"2024-08-30T16:31:54.899088Z","shell.execute_reply.started":"2024-08-30T16:31:54.875301Z","shell.execute_reply":"2024-08-30T16:31:54.897692Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"         0       1       2       3       4       5       6       7       8   \\\n0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n\n         9   ...      50      51      52      53      54      55      56  \\\n0    0.2111  ...  0.0232  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180   \n1    0.2872  ...  0.0125  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140   \n2    0.6194  ...  0.0033  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316   \n3    0.1264  ...  0.0241  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050   \n4    0.4459  ...  0.0156  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072   \n..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n203  0.2684  ...  0.0203  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065   \n204  0.2154  ...  0.0051  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034   \n205  0.2529  ...  0.0155  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140   \n206  0.2354  ...  0.0042  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034   \n207  0.2354  ...  0.0181  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040   \n\n         57      58      59  \n0    0.0084  0.0090  0.0032  \n1    0.0049  0.0052  0.0044  \n2    0.0164  0.0095  0.0078  \n3    0.0044  0.0040  0.0117  \n4    0.0048  0.0107  0.0094  \n..      ...     ...     ...  \n203  0.0115  0.0193  0.0157  \n204  0.0032  0.0062  0.0067  \n205  0.0138  0.0077  0.0031  \n206  0.0079  0.0036  0.0048  \n207  0.0036  0.0061  0.0115  \n\n[208 rows x 60 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(Y) #our target ","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:32:12.107612Z","iopub.execute_input":"2024-08-30T16:32:12.108115Z","iopub.status.idle":"2024-08-30T16:32:12.115777Z","shell.execute_reply.started":"2024-08-30T16:32:12.108064Z","shell.execute_reply":"2024-08-30T16:32:12.114578Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"0      R\n1      R\n2      R\n3      R\n4      R\n      ..\n203    M\n204    M\n205    M\n206    M\n207    M\nName: 60, Length: 208, dtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Training and testing","metadata":{}},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, stratify=Y, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:38:28.399331Z","iopub.execute_input":"2024-08-30T16:38:28.399799Z","iopub.status.idle":"2024-08-30T16:38:28.411278Z","shell.execute_reply.started":"2024-08-30T16:38:28.399756Z","shell.execute_reply":"2024-08-30T16:38:28.410094Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"test size : 10% of the data will be used for the test (bc our data is small)\nstratify parametr so thAT  we guarante that the labels are slited equaly\nrandom state : the choice of the number doesn t matter, but if yu keep the same number as mine, yur data will be splitted the same as mine and yu ll have the same results .","metadata":{}},{"cell_type":"code","source":"print(X.shape, X_train.shape, X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:38:32.173203Z","iopub.execute_input":"2024-08-30T16:38:32.173663Z","iopub.status.idle":"2024-08-30T16:38:32.180244Z","shell.execute_reply.started":"2024-08-30T16:38:32.173619Z","shell.execute_reply":"2024-08-30T16:38:32.178695Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"(208, 60) (187, 60) (21, 60)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(X_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:39:29.012954Z","iopub.execute_input":"2024-08-30T16:39:29.013451Z","iopub.status.idle":"2024-08-30T16:39:29.034872Z","shell.execute_reply.started":"2024-08-30T16:39:29.013395Z","shell.execute_reply":"2024-08-30T16:39:29.033634Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"         0       1       2       3       4       5       6       7       8   \\\n115  0.0414  0.0436  0.0447  0.0844  0.0419  0.1215  0.2002  0.1516  0.0818   \n38   0.0123  0.0022  0.0196  0.0206  0.0180  0.0492  0.0033  0.0398  0.0791   \n56   0.0152  0.0102  0.0113  0.0263  0.0097  0.0391  0.0857  0.0915  0.0949   \n123  0.0270  0.0163  0.0341  0.0247  0.0822  0.1256  0.1323  0.1584  0.2017   \n18   0.0270  0.0092  0.0145  0.0278  0.0412  0.0757  0.1026  0.1138  0.0794   \n..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n140  0.0412  0.1135  0.0518  0.0232  0.0646  0.1124  0.1787  0.2407  0.2682   \n5    0.0286  0.0453  0.0277  0.0174  0.0384  0.0990  0.1201  0.1833  0.2105   \n154  0.0117  0.0069  0.0279  0.0583  0.0915  0.1267  0.1577  0.1927  0.2361   \n131  0.1150  0.1163  0.0866  0.0358  0.0232  0.1267  0.2417  0.2661  0.4346   \n203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n\n         9   ...      50      51      52      53      54      55      56  \\\n115  0.1975  ...  0.0222  0.0045  0.0136  0.0113  0.0053  0.0165  0.0141   \n38   0.0475  ...  0.0149  0.0125  0.0134  0.0026  0.0038  0.0018  0.0113   \n56   0.1504  ...  0.0048  0.0049  0.0041  0.0036  0.0013  0.0046  0.0037   \n123  0.2122  ...  0.0197  0.0189  0.0204  0.0085  0.0043  0.0092  0.0138   \n18   0.1520  ...  0.0045  0.0084  0.0010  0.0018  0.0068  0.0039  0.0120   \n..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n140  0.2058  ...  0.0798  0.0376  0.0143  0.0272  0.0127  0.0166  0.0095   \n5    0.3039  ...  0.0104  0.0045  0.0014  0.0038  0.0013  0.0089  0.0057   \n154  0.2169  ...  0.0039  0.0053  0.0029  0.0020  0.0013  0.0029  0.0020   \n131  0.5378  ...  0.0228  0.0099  0.0065  0.0085  0.0166  0.0110  0.0190   \n203  0.2684  ...  0.0203  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065   \n\n         57      58      59  \n115  0.0077  0.0246  0.0198  \n38   0.0058  0.0047  0.0071  \n56   0.0011  0.0034  0.0033  \n123  0.0094  0.0105  0.0093  \n18   0.0132  0.0070  0.0088  \n..      ...     ...     ...  \n140  0.0225  0.0098  0.0085  \n5    0.0027  0.0051  0.0062  \n154  0.0062  0.0026  0.0052  \n131  0.0141  0.0068  0.0086  \n203  0.0115  0.0193  0.0157  \n\n[187 rows x 60 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"\nprint(Y_train)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:39:37.795430Z","iopub.execute_input":"2024-08-30T16:39:37.795921Z","iopub.status.idle":"2024-08-30T16:39:37.803593Z","shell.execute_reply.started":"2024-08-30T16:39:37.795877Z","shell.execute_reply":"2024-08-30T16:39:37.802281Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"115    M\n38     R\n56     R\n123    M\n18     R\n      ..\n140    M\n5      R\n154    M\n131    M\n203    M\nName: 60, Length: 187, dtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Model training**","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression()","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:40:21.105315Z","iopub.execute_input":"2024-08-30T16:40:21.105793Z","iopub.status.idle":"2024-08-30T16:40:21.111907Z","shell.execute_reply.started":"2024-08-30T16:40:21.105746Z","shell.execute_reply":"2024-08-30T16:40:21.110482Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#training the Logistic Regression model with training data : fit()\nmodel.fit(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:40:53.010917Z","iopub.execute_input":"2024-08-30T16:40:53.012068Z","iopub.status.idle":"2024-08-30T16:40:53.057547Z","shell.execute_reply.started":"2024-08-30T16:40:53.011992Z","shell.execute_reply":"2024-08-30T16:40:53.056323Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"LogisticRegression()","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Model evaluation**\n","metadata":{}},{"cell_type":"code","source":"#accuracy on test data : predict()\n\nX_test_prediction = model.predict(X_test)\ntest_data_accuracy = accuracy_score(X_test_prediction, Y_test) ","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:43:53.054335Z","iopub.execute_input":"2024-08-30T16:43:53.054795Z","iopub.status.idle":"2024-08-30T16:43:53.064639Z","shell.execute_reply.started":"2024-08-30T16:43:53.054754Z","shell.execute_reply":"2024-08-30T16:43:53.063120Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"print('Accuracy on test data : ', test_data_accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:44:04.474761Z","iopub.execute_input":"2024-08-30T16:44:04.475256Z","iopub.status.idle":"2024-08-30T16:44:04.481206Z","shell.execute_reply.started":"2024-08-30T16:44:04.475202Z","shell.execute_reply":"2024-08-30T16:44:04.479999Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Accuracy on test data :  0.7619047619047619\n","output_type":"stream"}]},{"cell_type":"markdown","source":"note : this accuracy is actually very good ! bc we used the model with it s default parametrs . if yu want to imporve it yu can add paramaetrs and change it\n","metadata":{}},{"cell_type":"markdown","source":"\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False","metadata":{}},{"cell_type":"markdown","source":"**The predictive system based on our model !**","metadata":{}},{"cell_type":"code","source":"input_data = (0.0307,0.0523,0.0653,0.0521,0.0611,0.0577,0.0665,0.0664,0.1460,0.2792,0.3877,0.4992,0.4981,0.4972,0.5607,0.7339,0.8230,0.9173,0.9975,0.9911,0.8240,0.6498,0.5980,0.4862,0.3150,0.1543,0.0989,0.0284,0.1008,0.2636,0.2694,0.2930,0.2925,0.3998,0.3660,0.3172,0.4609,0.4374,0.1820,0.3376,0.6202,0.4448,0.1863,0.1420,0.0589,0.0576,0.0672,0.0269,0.0245,0.0190,0.0063,0.0321,0.0189,0.0137,0.0277,0.0152,0.0052,0.0121,0.0124,0.0055)\n#this example normaly is a MINE !LETS SEE WHAT OUR MODEL WILL GIVE\n\n# changing the input_data to a numpy array\ninput_data_as_numpy_array = np.asarray(input_data)\n\n# reshape the np array as we are predicting for one instance\ninput_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n\nprediction = model.predict(input_data_reshaped)\nprint(prediction)\n\nif (prediction[0]=='R'):\n  print('The object is a Rock')\nelse:\n  print('The object is a mine')","metadata":{"execution":{"iopub.status.busy":"2024-08-30T17:01:41.599757Z","iopub.execute_input":"2024-08-30T17:01:41.600208Z","iopub.status.idle":"2024-08-30T17:01:41.611931Z","shell.execute_reply.started":"2024-08-30T17:01:41.600167Z","shell.execute_reply":"2024-08-30T17:01:41.610315Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"['M']\nThe object is a mine\n","output_type":"stream"}]}]}